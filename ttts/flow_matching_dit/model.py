import math
import torch
import torch.nn as nn
import torch.nn.functional as F

from ttts.flow_matching_dit.flow_matching import CFMDecoder
from ttts.flow_matching_dit.reference_encoder import MelStyleEncoder
from ttts.flow_matching_dit.mask import sequence_mask

TACOTRON_MEL_MAX = 5.5451774444795624753378569716654


def normalize_tacotron_mel(mel):
    mel = torch.clamp(mel, min=-TACOTRON_MEL_MAX)
    return mel*0.18215


# modified from https://github.com/shivammehta25/Matcha-TTS/blob/main/matcha/models/matcha_tts.py
class StableTTS(nn.Module):
    def __init__(self, mel_channels, hidden_channels, filter_channels, n_heads, n_dec_layers, kernel_size, p_dropout, gin_channels, cfg_dropout):
        super().__init__()

        self.mel_channels = mel_channels

        self.ref_encoder = MelStyleEncoder(mel_channels, style_vector_dim=gin_channels, style_kernel_size=5, dropout=0.25)
        self.decoder = CFMDecoder(mel_channels, mel_channels, hidden_channels, mel_channels, filter_channels, n_heads, n_dec_layers, kernel_size, p_dropout, gin_channels)
        
        # uncondition input for cfg
        self.fake_speaker = nn.Parameter(torch.zeros(1, gin_channels))
        self.fc = torch.nn.Conv1d(512, mel_channels, 1)

        self.cfg_dropout = cfg_dropout

    @torch.inference_mode()
    def synthesise(self, x, x_lengths, n_timesteps, temperature=1.0, y=None, length_scale=1.0, solver=None, cfg=1.0):
        """
        Generates mel-spectrogram from text. Returns:
            1. encoder outputs
            2. decoder outputs
            3. generated alignment

        Args:
            x (torch.Tensor): batch of texts, converted to a tensor with phoneme embedding ids.
                shape: (batch_size, max_text_length)
            x_lengths (torch.Tensor): lengths of texts in batch.
                shape: (batch_size,)
            n_timesteps (int): number of steps to use for reverse diffusion in decoder.
            temperature (float, optional): controls variance of terminal distribution.
            y (torch.Tensor): mel spectrogram of reference audio
                shape: (batch_size, mel_channels, time)
            length_scale (float, optional): controls speech pace.
                Increase value to slow down generated speech and vice versa.

        Returns:
            dict: {
                "encoder_outputs": torch.Tensor, shape: (batch_size, n_feats, max_mel_length),
                # Average mel spectrogram generated by the encoder
                "decoder_outputs": torch.Tensor, shape: (batch_size, n_feats, max_mel_length),
                # Refined mel spectrogram improved by the CFM
                "attn": torch.Tensor, shape: (batch_size, max_text_length, max_mel_length),
                # Alignment map between text and mel spectrogram
        """

        # Get encoder_outputs `mu_x` and log-scaled token durations `logw`
        c = self.ref_encoder(y, None)
        x = self.fc(x)
        mu_y = F.interpolate(x, size=x.shape[-1] * 4, mode='nearest')
        y_mask = sequence_mask(x_lengths, x.size(1)).unsqueeze(1).to(x.dtype)
        y_mask = F.interpolate(y_mask, size=mu_y.shape[-1], mode='nearest')
        y_max_length = x_lengths.max() * 4
        # Generate sample tracing the probability flow
        if cfg == 1.0:
            decoder_outputs = self.decoder(mu_y, y_mask, n_timesteps, temperature, c, solver)
        else:
            cfg_kwargs = {'fake_speaker': self.fake_speaker, 'fake_content': self.fake_content, 'cfg_strength': cfg}
            decoder_outputs = self.decoder(mu_y, y_mask, n_timesteps, temperature, c, solver, cfg_kwargs)
            
        decoder_outputs = decoder_outputs[:, :, :y_max_length]

        return decoder_outputs

    def forward(self, x, y, y_lengths, z, z_lengths):
        """
        Computes 3 losses:
            1. duration loss: loss between predicted token durations and those extracted by Monotinic Alignment Search (MAS).
            2. prior loss: loss between mel-spectrogram and encoder outputs.
            3. flow matching loss: loss between mel-spectrogram and decoder outputs.

        Args:
            x (torch.Tensor): batch of texts, converted to a tensor with phoneme embedding ids.
                shape: (batch_size, max_text_length)
            y (torch.Tensor): batch of corresponding mel-spectrograms.
                shape: (batch_size, n_feats, max_mel_length)
            y_lengths (torch.Tensor): lengths of mel-spectrograms in batch.
                shape: (batch_size,)
            z (torch.Tensor): batch of cliced mel-spectrograms.
                shape: (batch_size, n_feats, max_mel_length)
            z_lengths (torch.Tensor): lengths of sliced mel-spectrograms in batch.
                shape: (batch_size,)
        """
        # Get encoder_outputs `mu_x` and log-scaled token durations `logw`
        x = self.fc(x)
#       out = F.interpolate(x, size=y.shape[-1], mode='nearest')
#       print(out.shape)
#       loss = torch.nn.functional.mse_loss(out, y, reduction="sum")
#       return loss
        y_mask = sequence_mask(y_lengths, y.size(2)).unsqueeze(1).to(y.dtype)
        z_mask = sequence_mask(z_lengths, z.size(2)).unsqueeze(1).to(z.dtype)
        cfg_mask = torch.rand(y.size(0), 1, device=y.device) > self.cfg_dropout
        
        # compute global speaker embedding
        c = self.ref_encoder(z, z_mask) * cfg_mask + ~cfg_mask * self.fake_speaker.repeat(z.size(0), 1)
        
        mu_y_masked = F.interpolate(x, size=y.shape[-1], mode='nearest')
        diff_loss, _ = self.decoder.compute_loss(y, y_mask, mu_y_masked, c)
        
        return diff_loss
